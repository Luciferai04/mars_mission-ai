# Mars Mission AI - Autonomous Mission Planning System

## Table of Contents

- [Overview](#overview)
- [Technical Innovations](#technical-innovations)
- [System Architecture](#system-architecture)
- [Features](#features)
- [Machine Learning Models](#machine-learning-models)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Performance Metrics](#performance-metrics)
- [Development](#development)
- [Deployment](#deployment)
- [Contributing](#contributing)

---

## Overview

Mars Mission AI is an advanced autonomous mission planning system that combines computer vision, multi-agent reinforcement learning, and data integration to optimize Mars rover operations. The system uses state-of-the-art AI to analyze terrain, detect hazards, prioritize science targets, and generate optimal mission plans under constraints.

### Key Capabilities

- **Autonomous Terrain Analysis**: Real-time hazard detection and terrain classification
- **Multi-Agent Optimization**: Collaborative RL agents for complex mission planning
- **Resource Management**: Power and time budget optimization
- **Science Prioritization**: Intelligent target selection based on scientific value
- **Microservices Architecture**: Scalable, resilient distributed system
- **Real-time Data Integration**: Aggregation from multiple Mars data sources

---

## What's New (v2.0 â†’ v3.0)

- DQN backend for MARL (enable with `MARL_ALGO=dqn`); improved action quality using Double DQN and a hazard-aware heuristic fallback when Q-values are flat.
- Multi-rover coordination: `POST /coordinate_fleet` for joint actions across rovers.
- Predictive maintenance: `POST /maintenance/train`, `POST /maintenance/predict`.
- Enhanced vision: optical-flow object tracking `POST /track_objects` (OpenCV); hazard classifier endpoint `POST /classify_hazard`.
- Real-time streaming: WebSocket `GET /ws/telemetry`, publisher `POST /publish/telemetry`.
- Mobile app (Expo) for mission monitoring; connects to the telemetry WebSocket.
- 3D terrain visualization service at http://localhost:8006 with `/heightmap` and Three.js viewer.
- Voice command interface: `POST /command/execute` routes to the planner.
- Federated learning (FedAvg): `POST /federated/update`, `GET /federated/aggregate`.
- Autonomous experiment design: `POST /experiments/propose`.
- Long-term strategic planning: `POST /plan/long-term`.
- JPL planning tools export: `POST /export/jpl?fmt=plexil|apgen`.
- High-quality LaTeX/TikZ diagrams added: see `docs/tikz_diagrams.tex`, `docs/tikz_diagrams.pdf`, and `docs/TIKZ_DIAGRAMS_GUIDE.md`.
- CI updated with Docker Compose smoke tests; Git LFS is used for large model/data files.

See `docs/ROADMAP_IMPLEMENTATION.md` for a concise mapping of features to code.

## Technical Innovations

### 1. Multi-Agent Reinforcement Learning (MARL)

**Innovation**: Five specialized RL agents collaborating through weighted voting

**Agents**:
- **Route Planner Agent**: Optimizes navigation paths considering terrain and distance
- **Power Manager Agent**: Manages battery consumption and solar energy
- **Science Agent**: Prioritizes high-value scientific targets
- **Hazard Avoidance Agent**: Identifies and avoids dangerous terrain
- **Strategic Coordinator**: Long-term planning and resource allocation

**Technical Details**:
- Q-Learning with epsilon-greedy exploration
- Experience replay buffer (10,000 experiences)
- Weighted voting coordination mechanism
- State space: 8 dimensions (position, battery, time, environment)
- Action space: 6 discrete actions
- Training: 500+ episodes with convergence monitoring

**Performance**:
- 92%+ confidence after training
- Average reward: 750+ per episode
- Epsilon decay: 0.995 per episode

### 2. Vision-Based Terrain Analysis

**Innovation**: SegFormer transformer model for Mars terrain understanding

**Capabilities**:
- Semantic segmentation of terrain features
- Rock and obstacle detection with confidence scores
- Hazard classification (rocks, slopes, shadows)
- Real-time image preprocessing and normalization

**Technical Details**:
- Model: `nvidia/segformer-b0-finetuned-ade-512-512`
- Input resolution: 512x512 pixels
- Color space normalization for Mars-specific imagery
- GPU acceleration support

**Output**:
- Detected objects with bounding boxes
- Confidence scores per detection
- Hazard type classification
- Terrain traversability assessment

### 3. Microservices Architecture

**Innovation**: Domain-driven design with service mesh

**Services**:
1. **API Gateway (Kong)**: Unified entry point with rate limiting
2. **Vision Service**: Computer vision and image analysis
3. **MARL Service**: Reinforcement learning optimization
4. **Data Integration Service**: Multi-source data aggregation
5. **Planning Service**: Orchestration and mission coordination

**Benefits**:
- Independent scaling per service
- Fault isolation and resilience
- Technology stack flexibility
- Simplified deployment and updates

### 4. Intelligent Data Integration

**Innovation**: Asynchronous multi-source data fusion

**Data Sources**:
- Mars environmental sensors (REMS)
- Terrain elevation models (MOLA, HiRISE)
- Science target databases
- Rover telemetry and status

**Technical Features**:
- Concurrent async/await data fetching
- Redis caching layer
- PostgreSQL for persistent storage
- Real-time data validation

### 5. Adaptive Mission Planning

**Innovation**: Context-aware dynamic replanning

**Features**:
- Real-time constraint satisfaction
- Multi-objective optimization
- Risk assessment and mitigation
- Temporal planning with deadlines
- Resource-aware scheduling

---

## High-Quality Diagrams (TikZ/LaTeX)

All architecture and system diagrams are authored in LaTeX/TikZ for publication-grade quality.
- Source: `docs/tikz_diagrams.tex`
- Guide: `docs/TIKZ_DIAGRAMS_GUIDE.md`
- Compiled PDF (8 pages): `docs/tikz_diagrams.pdf`

Basic compilation (requires LaTeX):
```bash
pdflatex -interaction=nonstopmode -halt-on-error -output-directory=docs docs/tikz_diagrams.tex
```

To export SVG/PNG, see the guide for `dvisvgm`/`inkscape` workflows and CI tips.

## System Architecture

### High-Level Architecture

```mermaid
graph TB
    Client[Client Applications]
    Gateway[Kong API Gateway]
    
    subgraph "Microservices Layer"
        Planning[Planning Service<br/>Port 8005]
        Vision[Vision Service<br/>Port 8002]
        MARL[MARL Service<br/>Port 8003]
        Data[Data Integration<br/>Port 8004]
    end
    
    subgraph "Data Layer"
        Postgres[(PostgreSQL<br/>Port 5432)]
        Redis[(Redis Cache<br/>Port 6379)]
    end
    
    subgraph "ML Models"
        SegFormer[SegFormer<br/>Vision Model]
        Agents[5 RL Agents<br/>Trained Models]
    end
    
    Client --> Gateway
    Gateway --> Planning
    Gateway --> Vision
    Gateway --> MARL
    Gateway --> Data
    
    Planning --> Vision
    Planning --> MARL
    Planning --> Data
    
    Vision --> SegFormer
    MARL --> Agents
    Data --> Postgres
    Data --> Redis
    
    style Planning fill:#4A90E2
    style Vision fill:#50C878
    style MARL fill:#FF6B6B
    style Data fill:#FFD93D
```

### MARL System Architecture

```mermaid
graph LR
    subgraph "Mission Context"
        Input[Mission Parameters]
    end
    
    subgraph "MARL Agents"
        Route[Route Planner]
        Power[Power Manager]
        Science[Science Agent]
        Hazard[Hazard Avoidance]
        Strategy[Strategic Coordinator]
    end
    
    subgraph "Coordination"
        Vote[Weighted Voting]
        Consensus[Action Selection]
    end
    
    subgraph "Learning"
        QTable[Q-Tables]
        Replay[Experience Replay]
        Update[Q-Learning Update]
    end
    
    Input --> Route
    Input --> Power
    Input --> Science
    Input --> Hazard
    Input --> Strategy
    
    Route --> Vote
    Power --> Vote
    Science --> Vote
    Hazard --> Vote
    Strategy --> Vote
    
    Vote --> Consensus
    Consensus --> QTable
    Consensus --> Replay
    
    Replay --> Update
    Update --> QTable
    
    style Vote fill:#FF6B6B
    style QTable fill:#4A90E2
```

### Data Flow Architecture

```mermaid
sequenceDiagram
    participant Client
    participant Gateway
    participant Planning
    participant Data
    participant Vision
    participant MARL
    participant DB
    
    Client->>Gateway: POST /planning/plan
    Gateway->>Planning: Forward request
    
    Planning->>Data: GET /integrated
    Data->>DB: Query data
    DB-->>Data: Return data
    Data-->>Planning: Integrated data
    
    opt With Vision
        Planning->>Vision: POST /analyze
        Vision-->>Planning: Terrain analysis
    end
    
    Planning->>MARL: POST /optimize
    MARL->>MARL: Run 5 agents
    MARL->>MARL: Weighted voting
    MARL-->>Planning: Optimized plan
    
    Planning-->>Gateway: Mission plan
    Gateway-->>Client: Response
```

### Deployment Architecture

```mermaid
graph TB
    subgraph "Docker Network: mars-network"
        subgraph "Gateway Layer"
            Kong[Kong Gateway<br/>Ports: 8000, 8001]
            KongDB[(Kong PostgreSQL)]
        end
        
        subgraph "Service Layer"
            Vision[Vision Service<br/>Container: mars-vision]
            MARL[MARL Service<br/>Container: mars-marl]
            DataInt[Data Integration<br/>Container: mars-data-integration]
            Planning[Planning Service<br/>Container: mars-planning]
        end
        
        subgraph "Data Layer"
            Postgres[(PostgreSQL<br/>Container: mars-postgres)]
            Redis[(Redis<br/>Container: mars-redis)]
        end
        
        subgraph "Volumes"
            PG_Data[postgres_data]
            Redis_Data[redis_data]
            Models[models/]
        end
    end
    
    Kong --> Vision
    Kong --> MARL
    Kong --> DataInt
    Kong --> Planning
    
    Planning --> Vision
    Planning --> MARL
    Planning --> DataInt
    
    DataInt --> Postgres
    DataInt --> Redis
    
    Postgres --> PG_Data
    Redis --> Redis_Data
    Vision --> Models
    MARL --> Models
    
    style Kong fill:#00758F
    style Planning fill:#4A90E2
```

### MARL Agent State-Action Space

```mermaid
stateDiagram-v2
    [*] --> Initialization
    Initialization --> ObserveState
    
    ObserveState --> SelectAction
    note right of ObserveState
        State: [lat, lon, battery,
        time, temp, dust, targets, sol]
    end note
    
    SelectAction --> ExecuteAction
    note right of SelectAction
        Actions: [Move Forward, Turn Left,
        Turn Right, Science, Wait, Recharge]
    end note
    
    ExecuteAction --> ReceiveReward
    ExecuteAction --> UpdateEnvironment
    
    UpdateEnvironment --> ReceiveReward
    
    ReceiveReward --> StoreExperience
    StoreExperience --> UpdateQTable
    
    UpdateQTable --> CheckTerminal
    
    CheckTerminal --> ObserveState: Continue
    CheckTerminal --> [*]: Episode End
```

---

## Features

### Core Features

#### 1. Terrain Vision Analysis
- **Semantic Segmentation**: Pixel-level terrain classification
- **Object Detection**: Rocks, obstacles, and landmarks
- **Hazard Identification**: Slopes, shadows, dangerous terrain
- **Confidence Scoring**: Reliability metrics for each detection
- **Real-time Processing**: Sub-second analysis on GPU

#### 2. Multi-Agent Mission Optimization
- **Collaborative Planning**: 5 specialized RL agents
- **Resource Optimization**: Battery and time management
- **Science Maximization**: Target prioritization
- **Risk Mitigation**: Hazard-aware path planning
- **Adaptive Behavior**: Learning from experience

#### 3. Data Integration
- **Environmental Data**: Temperature, pressure, wind, dust
- **Terrain Data**: Elevation, slope, roughness
- **Science Targets**: POIs with priority scores
- **Rover Status**: Battery, power, system health
- **Concurrent Fetching**: Parallel data retrieval

#### 4. Mission Planning
- **Complete Plans**: End-to-end mission orchestration
- **Vision-Enhanced**: Image-based planning
- **Constraint Satisfaction**: Time and power budgets
- **Multi-Objective**: Balance multiple goals
- **Real-time Replanning**: Dynamic adaptation

#### 5. Microservices Infrastructure
- **Service Mesh**: Kong API Gateway
- **Health Monitoring**: Service status tracking
- **Horizontal Scaling**: Independent service scaling
- **Load Balancing**: Request distribution
- **Rate Limiting**: API protection

### Advanced Features

#### Reinforcement Learning
- Experience replay for stable learning
- Epsilon-greedy exploration strategy
- Weighted voting for agent coordination
- Model persistence and hot-reload
- Training statistics and monitoring

#### Computer Vision
- Mars-specific color normalization
- Multi-scale feature extraction
- Confidence thresholding
- GPU acceleration
- Batch processing support

#### Data Management
- PostgreSQL for structured data
- Redis for caching
- Async I/O for performance
- Connection pooling
- Data validation

---

## Machine Learning Models

### Vision Hazard Classifier (ConvNeXt-Tiny)

- Current service model: ConvNeXt-Tiny classifier (`models/terrain_vision_convnext.pth`).
- Endpoints: `/classify_hazard`, `/classify_batch`.
- Normalization: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225].
- Classes: SAFE, CAUTION, HAZARD.

### SegFormer Vision Model (optional/experimental)

**Architecture**: Transformer-based semantic segmentation
- Hierarchical encoder with overlapping patch embeddings
- Lightweight MLP decoder
- Mix-FFN for position information

**Training Data**: ADE20K dataset (150 categories)
**Parameters**: ~13.7M (B0 variant)
**Input**: 512x512 RGB images
**Output**: Per-pixel class predictions

**Preprocessing**:
```python
- Resize to 512x512
- Normalize: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
- Mars color correction (optional)
```

**Performance**:
- Inference time: ~100ms on GPU
- Inference time: ~2s on CPU
- Accuracy: Terrain-dependent

### MARL Q-Learning Models

**Algorithm**: Q-Learning with experience replay

**State Representation** (8 dimensions):
1. Latitude position
2. Longitude position
3. Battery state of charge (%)
4. Time budget remaining (minutes)
5. Temperature (Celsius)
6. Dust opacity (tau)
7. Number of remaining targets
8. Sol (Martian day)

**Action Space** (6 discrete actions):
1. Move forward (1 meter)
2. Turn left (45 degrees)
3. Turn right (45 degrees)
4. Perform science (instrument use)
5. Wait (conserve power)
6. Recharge (solar charging)

**Reward Function**:
```
R = w1 * science_reward 
  - w2 * power_cost 
  - w3 * time_cost 
  - w4 * hazard_penalty 
  + w5 * efficiency_bonus
```

**Hyperparameters**:
- Learning rate (alpha): 0.1
- Discount factor (gamma): 0.95
- Initial epsilon: 1.0
- Epsilon decay: 0.995
- Minimum epsilon: 0.01
- Replay buffer size: 10,000
- Batch size: 32

**Training Process**:
1. Initialize Q-tables randomly
2. For each episode:
   - Generate random mission scenario
   - Each agent selects actions
   - Weighted voting determines final action
   - Execute action, observe reward
   - Store experience in replay buffer
   - Sample batch and update Q-values
3. Save models periodically
4. Evaluate on test scenarios

**Convergence**:
- Training episodes: 500
- Average reward stabilization: ~300 episodes
- Final epsilon: 0.05-0.1
- Model size: ~500KB per agent

---

## Installation

### Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- Python 3.10+ (for local development)
- 8GB RAM minimum (16GB recommended)
- GPU (optional, for Vision Service acceleration)

### Quick Start with Docker

```bash
# Clone repository
git clone https://github.com/yourusername/mars_mission-ai.git
cd mars_mission-ai

# If you use large files (models/data), ensure Git LFS is installed once per machine
# brew install git-lfs && git lfs install

# Build and start all services (enable DQN agents)
MARL_ALGO=dqn docker compose up -d --build

# Configure API Gateway (optional demo setup)
chmod +x scripts/configure_gateway.sh
./scripts/configure_gateway.sh

# Verify services
curl -s http://localhost:8005/services/status | jq .
```

#### Quick Run Checklist (copy-paste)

```bash
# 1) Launch services with DQN
MARL_ALGO=dqn docker compose up -d --build

# 2) Train for better policies, then hot-reload
. .venv/bin/activate && python scripts/train_marl.py --episodes 2000
curl -X POST http://localhost:8003/agents/reload

# 3) Stream telemetry (tests WebSocket + mobile)
curl -sX POST http://localhost:8004/publish/telemetry \
  -H 'Content-Type: application/json' \
  -d '{"battery_soc":72,"power_generation":150,"power_consumption":120,"temp_c":-60,"dust_opacity":0.4}'

# 4) Create a plan
curl -sX POST http://localhost:8005/plan \
  -H 'Content-Type: application/json' \
  -d '{"sol":1600,"lat":18.4447,"lon":77.4508,"battery_soc":0.65,"time_budget_min":480,"objectives":["traverse","image","sample"]}'

# 5) Coordinate a fleet
curl -sX POST http://localhost:8003/coordinate_fleet \
  -H 'Content-Type: application/json' \
  -d '{"rovers":[{"rover_id":"percy","lat":18.444,"lon":77.451,"battery_soc":0.6,"time_remaining":300},{"rover_id":"helicopter","lat":18.446,"lon":77.452,"battery_soc":0.8,"time_remaining":200}]}'

# 6) Voice command
curl -sX POST http://localhost:8007/command/execute \
  -H 'Content-Type: application/json' \
  -d '{"text":"plan mission","context":{"sol":1600,"lat":18.4447,"lon":77.4508}}'

# 7) 3D viewer + mobile
open http://localhost:8006
(cd apps/mobile && npm i && EXPO_USE_WATCHMAN=1 npm start)
```

### Local Development Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY="your-key"
export NASA_API_KEY="your-key"

# Initialize database
psql -U mars_admin -d mars_mission -f database/init.sql

# Train MARL agents (optional)
python scripts/train_marl.py --episodes 500

# Run tests
pytest tests/
```

### Service-by-Service Installation

```bash
# Vision Service
cd services/vision
docker build -t mars-vision .
docker run -p 8002:8002 -v ./models:/models mars-vision

# MARL Service
cd services/marl
docker build -t mars-marl .
docker run -p 8003:8003 -v ./models:/models/marl mars-marl

# Data Integration Service
cd services/data-integration
docker build -t mars-data .
docker run -p 8004:8004 mars-data

# Planning Service
cd services/planning
docker build -t mars-planning .
docker run -p 8005:8005 mars-planning
```

---

## Usage

### Creating a Mission Plan

#### Basic Plan (No Vision)

```bash
curl -X POST http://localhost:8005/plan \
  -H "Content-Type: application/json" \
  -d '{
    "sol": 1234,
    "lat": -4.5,
    "lon": 137.4,
    "battery_soc": 85.0,
    "time_budget_min": 180,
    "objectives": ["science", "exploration"],
    "constraints": {
      "max_distance_km": 0.5,
      "min_battery_reserve": 20.0
    }
  }'
```

**Response**:
```json
{
  "plan_id": "PLAN_1234567890",
  "status": "completed",
  "mission_context": {...},
  "integrated_data": {
    "environment": {
      "sol": 1234,
      "temp_c": -63.0,
      "wind_speed": 2.1,
      "pressure": 750.0,
      "uv_index": 2.5,
      "dust_opacity": 0.4
    },
    "terrain": {...},
    "science_targets": [...]
  },
  "optimized_actions": [
    {
      "action": "move_forward",
      "distance_m": 5.0,
      "heading_deg": 45.0,
      "power_cost_wh": 25.0,
      "time_min": 3.0,
      "agent_votes": {
        "route": 0.9,
        "power": 0.7,
        "hazard": 0.8
      }
    },
    {
      "action": "science",
      "target_id": "SCI_001",
      "instruments": ["mastcam", "chemcam"],
      "power_cost_wh": 45.0,
      "time_min": 15.0
    }
  ],
  "expected_completion": 145,
  "total_power": 250.5,
  "total_time": 145,
  "rl_confidence": 0.87
}
```

#### Vision-Enhanced Plan

```bash
curl -X POST http://localhost:8005/plan/with-vision \
  -F "sol=1234" \
  -F "lat=-4.5" \
  -F "lon=137.4" \
  -F "battery_soc=85.0" \
  -F "time_budget_min=180" \
  -F "image=@terrain_image.jpg"
```

### Direct Service Usage

#### Vision Service

```bash
# Analyze terrain image
curl -X POST http://localhost:8002/analyze \
  -F "file=@mars_terrain.jpg"

# Response
{
  "status": "success",
  "image_shape": [512, 512, 3],
  "detected_objects": [
    {
      "class": "rock",
      "confidence": 0.89,
      "bbox": [120, 150, 50, 40]
    }
  ],
  "hazards": [
    {
      "type": "rock",
      "confidence": 0.89,
      "severity": "medium"
    }
  ],
  "processing_time_ms": 145.3
}
```

#### MARL Service

```bash
# Get agent statistics
curl http://localhost:8003/agents/stats

# Response
{
  "episodes": 500,
  "avg_reward": 782.3,
  "agent_confidence": {
    "route": "95.2%",
    "power": "94.8%",
    "science": "96.1%",
    "hazard": "93.7%",
    "strategy": "95.5%"
  },
  "total_experiences": 48532
}

# Get agent confidence breakdown
curl http://localhost:8003/agents/confidence

# Reload trained models
curl -X POST http://localhost:8003/agents/reload
```

#### Data Integration Service

```bash
# Get environment data
curl "http://localhost:8004/environment/1234"

# Get terrain data
curl "http://localhost:8004/terrain?lat=-4.5&lon=137.4"

# Get science targets
curl "http://localhost:8004/science-targets?lat=-4.5&lon=137.4&radius_km=5"

# Get integrated data
curl "http://localhost:8004/integrated?sol=1234&lat=-4.5&lon=137.4"
```

### Python Client Example

```python
import requests
import json

class MarsPlanner:
    def __init__(self, base_url="http://localhost:8005"):
        self.base_url = base_url
    
    def create_plan(self, mission_params):
        response = requests.post(
            f"{self.base_url}/plan",
            json=mission_params
        )
        return response.json()
    
    def create_vision_plan(self, params, image_path):
        files = {'image': open(image_path, 'rb')}
        response = requests.post(
            f"{self.base_url}/plan/with-vision",
            files=files,
            data=params
        )
        return response.json()

# Usage
planner = MarsPlanner()

mission = {
    "sol": 1234,
    "lat": -4.5,
    "lon": 137.4,
    "battery_soc": 85.0,
    "time_budget_min": 180,
    "objectives": ["science", "exploration"]
}

plan = planner.create_plan(mission)
print(f"Plan ID: {plan['plan_id']}")
print(f"Confidence: {plan['rl_confidence']:.2%}")
print(f"Actions: {len(plan['optimized_actions'])}")
```

---

## API Documentation

### Updated/Additional Endpoints (v2.xâ€“v3.0)

- Vision Service (http://localhost:8002)
  - GET /health
  - GET /model_info
  - POST /classify_hazard (UploadFile)
  - POST /classify_batch (UploadFiles)
  - POST /track_objects (UploadFiles; OpenCV optical flow)
  - POST /reload_model
- MARL Service (http://localhost:8003)
  - POST /optimize
  - GET /agents/stats, GET /agents/confidence, POST /agents/reload
  - POST /coordinate_fleet
  - POST /federated/update, GET /federated/aggregate
- Data Integration (http://localhost:8004)
  - WS /ws/telemetry, POST /publish/telemetry
  - POST /maintenance/train, POST /maintenance/predict
  - GET /environment/{sol}, GET /terrain, GET /science-targets, GET /integrated
- Planning (http://localhost:8005)
  - POST /plan
  - POST /experiments/propose, POST /plan/long-term, POST /export/jpl

Notes:
- Vision-enhanced `/plan/with-vision` is experimental; use vision endpoints directly to enrich targets before calling `/plan`.

### Planning Service API

#### POST /plan
Create optimized mission plan

**Request Body**:
```json
{
  "sol": 1234,
  "lat": -4.5,
  "lon": 137.4,
  "battery_soc": 85.0,
  "time_budget_min": 180,
  "objectives": ["science"],
  "constraints": {}
}
```

**Response**: MissionPlan object

#### POST /plan/with-vision
Create mission plan with terrain image

**Form Data**:
- sol: integer
- lat: float
- lon: float
- battery_soc: float
- time_budget_min: integer
- image: file

**Response**: MissionPlan with vision_analysis

#### GET /services/status
Get status of all services

**Response**:
```json
{
  "planning_service": "operational",
  "services": {
    "vision": {"status": "healthy", ...},
    "marl": {"status": "healthy", ...},
    "data": {"status": "healthy", ...}
  }
}
```

### Vision Service API

#### POST /analyze
Analyze terrain image

**Form Data**: file (image)

**Response**: Terrain analysis with objects and hazards

#### GET /health
Health check

### MARL Service API

#### POST /optimize
Optimize mission using MARL

**Request Body**: Mission context

**Response**: Optimized actions with RL confidence

#### GET /agents/stats
Get training statistics

#### GET /agents/confidence
Get agent confidence levels

#### POST /agents/reload
Reload trained models

### Data Integration Service API

#### GET /environment/{sol}
Get environmental data for sol

#### GET /terrain?lat=X&lon=Y
Get terrain data for coordinates

#### GET /science-targets?lat=X&lon=Y&radius_km=Z
Get science targets in area

#### GET /integrated?sol=X&lat=Y&lon=Z
Get all integrated data

### Interactive Documentation

Once services are running:
- Vision: http://localhost:8002/docs
- MARL: http://localhost:8003/docs
- Data Integration: http://localhost:8004/docs
- Planning: http://localhost:8005/docs

---

## Performance Metrics

### MARL Training Performance

| Metric | Value |
|--------|-------|
| Training Episodes | 500 |
| Total Training Time | ~45 minutes |
| Final Average Reward | 782.3 |
| Convergence Episode | ~300 |
| Final Epsilon | 0.048 |
| Experience Buffer Size | 48,532 |

### Agent Confidence Scores

| Agent | Confidence | Epsilon |
|-------|-----------|---------|
| Route Planner | 95.2% | 0.048 |
| Power Manager | 94.8% | 0.052 |
| Science Agent | 96.1% | 0.039 |
| Hazard Avoidance | 93.7% | 0.063 |
| Strategic Coordinator | 95.5% | 0.045 |

### Vision Service Performance

| Operation | GPU Time | CPU Time |
|-----------|----------|----------|
| Image Preprocessing | 5ms | 20ms |
| Model Inference | 95ms | 1,800ms |
| Post-processing | 15ms | 30ms |
| **Total** | **115ms** | **1,850ms** |

### Service Response Times

| Service | P50 | P95 | P99 |
|---------|-----|-----|-----|
| Vision Service | 120ms | 200ms | 350ms |
| MARL Service | 50ms | 85ms | 120ms |
| Data Integration | 180ms | 320ms | 450ms |
| Planning Service | 800ms | 1,200ms | 1,800ms |

### Resource Utilization

| Service | CPU (avg) | Memory | Disk |
|---------|-----------|--------|------|
| Vision Service | 45% | 2.5GB | 1.2GB |
| MARL Service | 25% | 1.8GB | 500MB |
| Data Integration | 15% | 512MB | 100MB |
| Planning Service | 20% | 768MB | 50MB |
| PostgreSQL | 10% | 256MB | 2GB |
| Redis | 5% | 128MB | 500MB |

---

## System Visualizations

### High-Level Architecture
![High-Level Architecture](docs/screenshots/architecture_highlevel.png)

*Complete system architecture showing client, API gateway, microservices layer, data layer, and ML models*

### MARL System Architecture
![MARL System](docs/screenshots/marl_system.png)

*Multi-Agent RL system with 5 specialized agents using weighted voting coordination*

### Data Flow Sequence
![Data Flow](docs/screenshots/data_flow.png)

*Request/response flow through all microservices*

### Docker Deployment
![Deployment Architecture](docs/screenshots/deployment.png)

*Complete Docker container topology with network, volumes, and service layers*

### MARL Agent State-Action Space
![MARL State-Action](docs/screenshots/marl_stateaction.png)

*RL agent lifecycle showing state observation, action selection, and Q-learning updates*

### Training Performance
![Training Performance](docs/screenshots/training_performance.png)

*MARL training convergence over 500 episodes reaching 782.3 average reward*

### Service Performance Comparison
![Service Performance](docs/screenshots/service_performance.png)

*P95 response times across all microservices*

### Communication Patterns
![Communication](docs/screenshots/communication_pattern.png)

*Microservices communication showing synchronous REST and asynchronous messaging*

---

## Development

### Project Structure

```
mars_mission-ai/
 src/
    core/
       multi_agent_rl.py      # MARL system
       mission_planner.py     # Mission planning
       terrain_analyzer.py    # Vision analysis
    interfaces/
       marl_endpoint.py       # MARL API
       web_api.py             # Main API
    utils/
        logging_config.py
        validators.py
 services/
    vision/                    # Vision microservice
    marl/                      # MARL microservice
    data-integration/          # Data microservice
    planning/                  # Planning microservice
 models/
    marl/                      # Trained RL agents
    vision/                    # Vision models
 database/
    init.sql                   # DB initialization
    schema.sql                 # Database schema
 scripts/
    train_marl.py              # MARL training
    configure_gateway.sh       # Kong setup
 tests/
    test_marl.py
    test_vision.py
    test_integration.py
 docker-compose.yml             # Service orchestration
 MARL_SYSTEM.md                 # MARL documentation
 MICROSERVICES_DEPLOYMENT.md    # Deployment guide
 README.md                      # This file
```

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-asyncio

# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/test_marl.py -v

# Run integration tests
pytest tests/test_integration.py
```

### Training MARL Agents

```bash
# Train with default settings (500 episodes)
python scripts/train_marl.py

# Train with custom episodes
python scripts/train_marl.py --episodes 1000

# Test trained models
python scripts/train_marl.py --test

# Resume training from checkpoint
python scripts/train_marl.py --resume --episodes 200
```

### Code Quality

```bash
# Format code
black src/ services/ scripts/

# Lint code
pylint src/ services/

# Type checking
mypy src/

# Security scan
bandit -r src/
```

---

## CI/CD and Smoke Tests

- GitHub Actions workflow at `.github/workflows/ci.yml` runs:
  - Lint + unit tests on Python 3.10/3.11
  - Docker image build and (optional) push
  - Docker Compose smoke tests: builds stack, waits for health, calls `/plan` and `/coordinate_fleet`
- Set required secrets (e.g., `DOCKER_USERNAME`, `DOCKER_PASSWORD`, API keys) in repository settings.

## Deployment

### Production Deployment

#### Docker Swarm

```bash
# Initialize swarm
docker swarm init

# Deploy stack
docker stack deploy -c docker-compose.yml mars-mission

# Scale services
docker service scale mars-mission_marl-service=3
docker service scale mars-mission_vision-service=2

# Monitor services
docker service ls
docker service logs mars-mission_planning-service
```

#### Kubernetes

```bash
# Create namespace
kubectl create namespace mars-mission

# Deploy services
kubectl apply -f k8s/

# Scale deployment
kubectl scale deployment marl-service --replicas=3

# Check status
kubectl get pods -n mars-mission
kubectl logs -f deployment/planning-service -n mars-mission
```

#### Cloud Deployment (AWS)

```bash
# Build and push images
docker build -t your-registry/mars-vision:latest services/vision
docker push your-registry/mars-vision:latest

# Deploy with ECS/EKS
aws ecs create-cluster --cluster-name mars-mission
aws ecs register-task-definition --cli-input-json file://task-definition.json
aws ecs create-service --cluster mars-mission --service-name planning --task-definition mars-planning
```

### Environment Variables

```bash
# Core Configuration
export NASA_API_KEY="your-nasa-api-key"
export OPENAI_API_KEY="your-openai-key"

# Service URLs (for local development)
export VISION_SERVICE_URL="http://localhost:8002"
export MARL_SERVICE_URL="http://localhost:8003"
export DATA_SERVICE_URL="http://localhost:8004"

# Database Configuration
export DATABASE_URL="postgresql://mars_admin:password@localhost:5432/mars_mission"
export REDIS_URL="redis://localhost:6379"

# Model Paths
export MODEL_DIR="./models"
export MARL_MODEL_DIR="./models/marl"
export VISION_MODEL_PATH="./models/vision"

# Logging
export LOG_LEVEL="INFO"
export LOG_FILE="./logs/mars_mission.log"
```

### Monitoring and Observability

#### Prometheus Metrics

```yaml
# Add to docker-compose.yml
prometheus:
  image: prom/prometheus
  ports:
    - "9090:9090"
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
```

#### Grafana Dashboards

```yaml
grafana:
  image: grafana/grafana
  ports:
    - "3000:3000"
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=admin
```

#### Log Aggregation (ELK)

```yaml
elasticsearch:
  image: elasticsearch:8.8.0
  ports:
    - "9200:9200"

logstash:
  image: logstash:8.8.0
  
kibana:
  image: kibana:8.8.0
  ports:
    - "5601:5601"
```

---

## Contributing

### Development Workflow

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

### Coding Standards

- Follow PEP 8 style guide
- Use type hints for all functions
- Write docstrings for all public APIs
- Maintain test coverage above 80%
- Use meaningful variable names
- Keep functions under 50 lines
- Document complex algorithms

### Commit Message Format

```
type(scope): subject

body

footer
```

**Types**: feat, fix, docs, style, refactor, test, chore

**Example**:
```
feat(marl): add strategic coordinator agent

- Implement long-term planning logic
- Add weighted voting coordination
- Update training script with new agent

Closes #123
```

---

## License

This project is licensed under the MIT License - see LICENSE file for details.

---

## Acknowledgments

- NASA Mars Exploration Program
- Hugging Face Transformers library
- SegFormer paper authors
- OpenAI for API services
- Kong Gateway project
- FastAPI framework

---

## Contact and Support

- Issues: https://github.com/yourusername/mars_mission-ai/issues
- Documentation: https://mars-mission-ai.readthedocs.io
- Email: support@marsmission.ai

---

## Roadmap

### Version 2.0 (Q2 2024)
- Deep Q-Networks (DQN) for MARL
- Multi-rover coordination
- Predictive maintenance AI
- Enhanced vision with object tracking

### Version 2.1 (Q3 2024)
- Real-time streaming data integration
- Mobile app for mission monitoring
- 3D terrain visualization
- Voice command interface

### Version 3.0 (Q4 2024)
- Federated learning across rovers
- Autonomous experiment design
- Long-term strategic planning (sols to months)
- Integration with JPL planning tools

---

## Frequently Asked Questions

### Q: How accurate is the MARL system?
A: After 500 episodes of training, the system achieves 92-96% confidence in action selection, with an average reward of 782.3. Performance improves with more training.

### Q: Can I use custom vision models?
A: Yes, the Vision Service supports any Hugging Face model. Update the model name in the configuration.

### Q: How do I scale for production?
A: Use Docker Swarm or Kubernetes to scale services independently. The MARL and Vision services benefit most from horizontal scaling.

### Q: What hardware is recommended?
A: Minimum: 8GB RAM, 4 cores. Recommended: 16GB RAM, 8 cores, GPU for Vision Service.

### Q: How do I train on real Mars data?
A: Replace mock data in Data Integration Service with real APIs (MOLA, HiRISE, REMS). Update the training script with real mission scenarios.

### Q: Is this used by NASA?
A: This is an independent research project demonstrating AI techniques applicable to Mars missions. Not officially endorsed by NASA.

---

**Built with passion for Mars exploration**

Version: 1.0.0 | Last Updated: 2024
